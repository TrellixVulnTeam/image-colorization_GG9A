{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from skimage import io, color\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    \n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict[b\"data\"]\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "def load_data():\n",
    "    # Load training data\n",
    "    train = np.reshape(unpickle('cifar-10-batches-py/data_batch_1'), (10000, 3, 32, 32))\n",
    "    train = np.append(train, np.reshape(unpickle('cifar-10-batches-py/data_batch_2'), (10000, 3, 32, 32)), 0)\n",
    "    train = np.append(train, np.reshape(unpickle('cifar-10-batches-py/data_batch_3'), (10000, 3, 32, 32)), 0)\n",
    "    train = np.append(train, np.reshape(unpickle('cifar-10-batches-py/data_batch_4'), (10000, 3, 32, 32)), 0)\n",
    "    train = np.append(train, np.reshape(unpickle('cifar-10-batches-py/data_batch_5'), (10000, 3, 32, 32)), 0)\n",
    "\n",
    "    # Convert to greyscale and LAB\n",
    "    train_grey = np.zeros((50000, 1, 32, 32))\n",
    "    train_lab  = np.zeros((50000, 3, 32, 32))\n",
    "    for i in range(0, len(train)):\n",
    "        grey = np.dot(train[i].transpose(1,2,0), [0.299, 0.587, 0.114])\n",
    "        lab = color.rgb2lab(train[i].transpose(1,2,0)).transpose(2,0,1)\n",
    "        train_grey[i][0] = grey\n",
    "        train_lab[i] = lab\n",
    "    # Convert to 0-1 range to avoid tanh fuckery\n",
    "    train_lab = torch.tensor(train_lab/100).float()\n",
    "    train_grey = torch.tensor(train_grey/255).float()\n",
    "\n",
    "\n",
    "    # Load test and validation data\n",
    "    testvalid = np.reshape(unpickle('cifar-10-batches-py/test_batch'), (10000, 3, 32, 32))\n",
    "    valid = testvalid[0:9000]\n",
    "    test = testvalid[0:1000]\n",
    "\n",
    "    # Convert to greyscale and lab\n",
    "    valid_grey = np.zeros((9000, 1, 32, 32))\n",
    "    valid_lab = np.zeros((9000, 3, 32, 32))\n",
    "    for i in range(0, len(test)):\n",
    "        grey = np.dot(valid[i].transpose(1,2,0), [0.299, 0.587, 0.114])\n",
    "        valid_grey[i][0] = grey\n",
    "\n",
    "        lab = color.rgb2lab(valid[i].transpose(1,2,0)).transpose(2,0,1)\n",
    "        valid_lab[i] = lab\n",
    "    # Convert to 0-1 range to avoid tanh fuckery\n",
    "    valid_lab = torch.tensor(valid_lab/100).float()\n",
    "    valid_grey = torch.tensor(valid_grey/255).float()\n",
    "\n",
    "    # Convert to greyscale\n",
    "    test_grey = np.zeros((1000, 1, 32, 32))\n",
    "    for i in range(0, len(test)):\n",
    "        grey = np.dot(test[i].transpose(1,2,0), [0.299, 0.587, 0.114])\n",
    "        test_grey[i][0] = grey\n",
    "    # Convert to 0-1 range to avoid tanh fuckery\n",
    "    test_grey = torch.tensor(test_grey/255).float()\n",
    "    \n",
    "    return train_lab, train_grey, valid_lab, valid_grey, test, test_grey\n",
    "\n",
    "train_color, train_grey, valid_color, valid_grey, test_color, test_grey = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR_iterator():\n",
    "    def __init__(self, data_tuple, batch_size):\n",
    "        self.data_tuple = data_tuple\n",
    "        self.batch_size = batch_size\n",
    "        self.i = 0\n",
    "        self.iter = 0\n",
    "        self.iters = np.floor_divide(data_tuple[0].size(0), batch_size)\n",
    "        \n",
    "    def getNext(self):            \n",
    "        self.i += self.batch_size\n",
    "        self.iter += 1\n",
    "        res = (self.data_tuple[0][self.i:self.i +self.batch_size], self.data_tuple[1][self.i:self.i +self.batch_size])\n",
    "        return res\n",
    "    \n",
    "    def getIter(self):\n",
    "        return self.iter\n",
    "    \n",
    "    def getIters(self):\n",
    "        return self.iters\n",
    "    \n",
    "    def reset(self):\n",
    "        self.i = 0\n",
    "        self.iter = 0\n",
    "        \n",
    "def to_rgb(grayscale_input, ab_input, save_path=None, save_name=None):\n",
    "    '''Show/save rgb image from grayscale and ab channels\n",
    "       Input save_path in the form {'grayscale': '/path/', 'colorized': '/path/'}'''\n",
    "    plt.clf()  # clear matplotlib\n",
    "    color_image = ab_input.numpy()  # combine channels\n",
    "    color_image = color_image.transpose((1, 2, 0))  # rescale for matplotlib\n",
    "    color_image = color_image * 100\n",
    "    color_image = color.lab2rgb(color_image.astype(np.float64))\n",
    "    grayscale_input = grayscale_input.squeeze().numpy()\n",
    "    if save_path is not None and save_name is not None:\n",
    "        plt.imsave(arr=grayscale_input, fname='{}{}'.format(save_path['grayscale'], save_name), cmap='gray')\n",
    "        plt.imsave(arr=color_image, fname='{}{}'.format(save_path['colorized'], save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  loaded\n",
      "net initialised\n",
      "Starting training epoch 0\n",
      "Epoch: [0][24/781]\tTime 0.138 (0.171)\tData 0.000 (0.000)\tLoss 0.2936 (0.2828)\t\n",
      "Epoch: [0][49/781]\tTime 0.082 (0.167)\tData 0.000 (0.000)\tLoss 0.2786 (0.2767)\t\n",
      "Epoch: [0][74/781]\tTime 0.096 (0.162)\tData 0.000 (0.000)\tLoss 0.2496 (0.2718)\t\n",
      "Epoch: [0][99/781]\tTime 0.246 (0.172)\tData 0.000 (0.000)\tLoss 0.2498 (0.2669)\t\n",
      "Epoch: [0][124/781]\tTime 0.253 (0.175)\tData 0.000 (0.000)\tLoss 0.2467 (0.2622)\t\n",
      "Epoch: [0][149/781]\tTime 0.121 (0.169)\tData 0.000 (0.000)\tLoss 0.2411 (0.2581)\t\n",
      "Epoch: [0][174/781]\tTime 0.098 (0.163)\tData 0.000 (0.000)\tLoss 0.2243 (0.2538)\t\n",
      "Epoch: [0][199/781]\tTime 0.116 (0.161)\tData 0.000 (0.000)\tLoss 0.2161 (0.2491)\t\n",
      "Epoch: [0][224/781]\tTime 0.138 (0.157)\tData 0.000 (0.000)\tLoss 0.1945 (0.2446)\t\n",
      "Epoch: [0][249/781]\tTime 0.181 (0.152)\tData 0.000 (0.000)\tLoss 0.1909 (0.2398)\t\n",
      "Epoch: [0][274/781]\tTime 0.160 (0.156)\tData 0.000 (0.000)\tLoss 0.1866 (0.2354)\t\n",
      "Epoch: [0][299/781]\tTime 0.191 (0.156)\tData 0.000 (0.000)\tLoss 0.1708 (0.2309)\t\n",
      "Epoch: [0][324/781]\tTime 0.165 (0.154)\tData 0.000 (0.000)\tLoss 0.1691 (0.2265)\t\n",
      "Epoch: [0][349/781]\tTime 0.223 (0.152)\tData 0.000 (0.000)\tLoss 0.1667 (0.2221)\t\n",
      "Epoch: [0][374/781]\tTime 0.242 (0.152)\tData 0.000 (0.000)\tLoss 0.1486 (0.2177)\t\n",
      "Epoch: [0][399/781]\tTime 0.130 (0.153)\tData 0.000 (0.000)\tLoss 0.1494 (0.2136)\t\n",
      "Epoch: [0][424/781]\tTime 0.130 (0.154)\tData 0.000 (0.000)\tLoss 0.1434 (0.2094)\t\n",
      "Epoch: [0][449/781]\tTime 0.194 (0.155)\tData 0.000 (0.000)\tLoss 0.1330 (0.2052)\t\n",
      "Epoch: [0][474/781]\tTime 0.106 (0.154)\tData 0.000 (0.000)\tLoss 0.1296 (0.2012)\t\n",
      "Epoch: [0][499/781]\tTime 0.189 (0.156)\tData 0.000 (0.000)\tLoss 0.1193 (0.1974)\t\n",
      "Epoch: [0][524/781]\tTime 0.179 (0.158)\tData 0.000 (0.000)\tLoss 0.0976 (0.1934)\t\n",
      "Epoch: [0][549/781]\tTime 0.167 (0.160)\tData 0.000 (0.000)\tLoss 0.1010 (0.1894)\t\n",
      "Epoch: [0][574/781]\tTime 0.094 (0.160)\tData 0.000 (0.000)\tLoss 0.0924 (0.1856)\t\n",
      "Epoch: [0][599/781]\tTime 0.079 (0.157)\tData 0.000 (0.000)\tLoss 0.0844 (0.1818)\t\n",
      "Epoch: [0][624/781]\tTime 0.083 (0.154)\tData 0.000 (0.000)\tLoss 0.0843 (0.1781)\t\n",
      "Epoch: [0][649/781]\tTime 0.180 (0.155)\tData 0.000 (0.000)\tLoss 0.0800 (0.1745)\t\n",
      "Epoch: [0][674/781]\tTime 0.127 (0.157)\tData 0.000 (0.000)\tLoss 0.0719 (0.1710)\t\n",
      "Epoch: [0][699/781]\tTime 0.104 (0.158)\tData 0.000 (0.000)\tLoss 0.0717 (0.1675)\t\n",
      "Epoch: [0][724/781]\tTime 0.190 (0.158)\tData 0.000 (0.000)\tLoss 0.0693 (0.1640)\t\n",
      "Epoch: [0][749/781]\tTime 0.255 (0.158)\tData 0.000 (0.000)\tLoss 0.0657 (0.1606)\t\n",
      "Epoch: [0][774/781]\tTime 0.118 (0.160)\tData 0.000 (0.000)\tLoss 0.0586 (0.1574)\t\n",
      "Finished training epoch 0\n",
      "Finished validation.\n",
      "Starting training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dominik/miniconda3/envs/mlp/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 54 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/home/dominik/miniconda3/envs/mlp/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 94 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/home/dominik/miniconda3/envs/mlp/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 155 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/home/dominik/miniconda3/envs/mlp/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 6 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/home/dominik/miniconda3/envs/mlp/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 364 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/home/dominik/miniconda3/envs/mlp/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 34 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/home/dominik/miniconda3/envs/mlp/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 213 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/home/dominik/miniconda3/envs/mlp/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 19 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/home/dominik/miniconda3/envs/mlp/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 67 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2e0cd9058d36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2e0cd9058d36>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, model, criterion, optimiser, epoch)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Compute gradient and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unet, self).__init__()\n",
    "        #Convolution and deconvolution\n",
    "        self.conv1 = nn.Conv2d(1, 4, (4, 4), stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(4, 8, (4, 4), stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(8, 16, (4, 4), stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(16, 32, (4, 4), stride=2, padding=1)\n",
    "        #self.conv5 = nn.Conv2d(3, 3, (4, 4), stride=2, padding=1)\n",
    "        self.deconv1 = nn.ConvTranspose2d(32, 16, (4, 4), stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(32, 8, (4, 4), stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(16, 4, (4, 4), stride=2, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(8, 4, (4, 4), stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(4, 3, (1, 1))\n",
    "        \n",
    "        #Batchnorm\n",
    "        self.conv1_bnorm = nn.BatchNorm2d(4)\n",
    "        self.conv2_bnorm = nn.BatchNorm2d(8)\n",
    "        self.conv3_bnorm = nn.BatchNorm2d(16)\n",
    "        self.conv4_bnorm = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.deconv1_bnorm = nn.BatchNorm2d(16)\n",
    "        self.deconv2_bnorm = nn.BatchNorm2d(8)\n",
    "        self.deconv3_bnorm = nn.BatchNorm2d(4)\n",
    "        self.deconv4_bnorm = nn.BatchNorm2d(4)\n",
    "    \n",
    "    def forward(self, x32):\n",
    "        # Contraction\n",
    "        x16 = F.leaky_relu(self.conv1(x32), 0.2)\n",
    "        x16 = self.conv1_bnorm(x16)\n",
    "        \n",
    "        x8 = F.leaky_relu(self.conv2(x16), 0.2)\n",
    "        x8 = self.conv2_bnorm(x8)\n",
    "        \n",
    "        x4 = F.leaky_relu(self.conv3(x8), 0.2)\n",
    "        x4 = self.conv3_bnorm(x4)\n",
    "        \n",
    "        x2 = F.leaky_relu(self.conv4(x4), 0.2)\n",
    "        x2 = self.conv4_bnorm(x2)\n",
    "        \n",
    "        \n",
    "        # Expansion\n",
    "        x = F.relu(self.deconv1(x2))\n",
    "        x = self.deconv1_bnorm(x)\n",
    "        x4 = torch.cat((x,x4), 1)\n",
    "        \n",
    "        x = F.relu(self.deconv2(x4))\n",
    "        x = self.deconv2_bnorm(x)\n",
    "        x8 = torch.cat((x,x8), 1)\n",
    "        \n",
    "        x = F.relu(self.deconv3(x8))\n",
    "        x = self.deconv3_bnorm(x)\n",
    "        x16 = torch.cat((x,x16), 1)\n",
    "        \n",
    "        x = F.relu(self.deconv4(x16))\n",
    "        x = self.deconv4_bnorm(x)\n",
    "        \n",
    "        # cross-channel parametric pooling\n",
    "        # CHECK IF TANH IS A GOOD IDEA???\n",
    "        x = F.tanh(self.conv5(x))\n",
    "        #x = self.conv5(x)\n",
    "        return x\n",
    "\n",
    "# To track training loss\n",
    "class AverageMeter(object):\n",
    "    '''A handy class from the PyTorch ImageNet tutorial'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def write_results_to_file(file_dir, file_name, data):\n",
    "    file = open(file_dir + os.path.sep + file_name, 'a')\n",
    "    if isinstance(data, str):\n",
    "        file.write(data + '\\n')\n",
    "    else:\n",
    "        for line in data:\n",
    "            file.write(line + '\\n')\n",
    "    file.close()\n",
    "\n",
    "def train_model(train_loader, model, criterion, optimiser, epoch):\n",
    "    print('Starting training epoch {}'.format(epoch))\n",
    "    model.train()\n",
    "    \n",
    "    dir_name = \"res\"\n",
    "    file_name = \"intermediate\"\n",
    "    \n",
    "    # Prepare value counters and timers\n",
    "    batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    for i in range(0, train_loader.getIters()):\n",
    "        \n",
    "        (input_gray, target) = train_loader.getNext()\n",
    "        # use gpu\n",
    "        if use_gpu: input_gray, target = input_gray.cuda(), target.cuda()\n",
    "        \n",
    "        # Record load time data\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        # Run forward pass\n",
    "        output_ab = model(input_gray)\n",
    "        loss = criterion(output_ab, target)\n",
    "        losses.update(loss.item(), input_gray.size(0))\n",
    "        \n",
    "        # Compute gradient and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "         # Record time to do forward and backward passes\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        # Print model accuracy -- in the code below, val refers to value, not validation\n",
    "        if train_loader.getIter() % 25 == 0:\n",
    "            stats = (\n",
    "                'Epoch: [{0}][{1}/{2}]\\tTime {batch_time.val:.3f} ({batch_time.avg:.3f})\\tData {data_time.val:.3f} ({'\n",
    "                'data_time.avg:.3f})\\tLoss {loss.val:.4f} ({loss.avg:.4f})\\t').format(\n",
    "                epoch, i, train_loader.getIters(), batch_time=batch_time,\n",
    "                data_time=data_time, loss=losses)\n",
    "            print(stats)\n",
    "            write_results_to_file(dir_name, file_name, stats)\n",
    "            \n",
    "    train_loader.reset()\n",
    "\n",
    "    print('Finished training epoch {}'.format(epoch))\n",
    "    \n",
    "def validate(val_loader, model, criterion, save_images, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare value counters and timers\n",
    "    batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    already_saved_images = False\n",
    "    for i in range(0, 1):\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        (input_gray, target) = val_loader.getNext()\n",
    "        # use gpu\n",
    "        if use_gpu: input_gray, target = input_gray.cuda(), target.cuda()\n",
    "\n",
    "        # Run model and record loss\n",
    "        output_ab = model(input_gray)  # throw away class predictions\n",
    "        loss = criterion(output_ab, target)\n",
    "        losses.update(loss.item(), input_gray.size(0))\n",
    "\n",
    "        # Save images to file\n",
    "        if save_images and not already_saved_images:\n",
    "            already_saved_images = True\n",
    "            for j in range(min(len(output_ab), 10)):  # save at most 5 images\n",
    "                save_path = {'grayscale': 'res/grey/', 'colorized': 'res/color/'}\n",
    "                save_name = 'img-{}-epoch-{}.jpg'.format(i * val_loader.batch_size + j, epoch)\n",
    "                to_rgb(input_gray[j].cpu(), ab_input=output_ab[j].detach().cpu(), save_path=save_path,\n",
    "                       save_name=save_name)\n",
    "\n",
    "        # Record time to do forward passes and save images\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "    val_loader.reset()\n",
    "    print('Finished validation.')\n",
    "    return losses.avg\n",
    "\n",
    "# Load and process the inputs and targets\n",
    "#targets, inputs, valid_targets, valid_inputs, test_inputs = load_data()\n",
    "print(\"data  loaded\")\n",
    "\n",
    "# Make net\n",
    "net = Unet()\n",
    "print(\"net initialised\")\n",
    "\n",
    "# Ensure res directory exists\n",
    "os.makedirs('res', exist_ok=True)\n",
    "os.makedirs('res/grey', exist_ok=True)\n",
    "os.makedirs('res/color/', exist_ok=True)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0002, betas=(0.5,0.999))\n",
    "criterion = torch.nn.MSELoss()\n",
    "epochs = 50\n",
    "save_images = True\n",
    "\n",
    "train_loader = CIFAR_iterator((train_grey, train_color), 64)\n",
    "val_loader = CIFAR_iterator((valid_grey, valid_color), 64)\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train_model(train_loader, net, criterion, optimizer, epoch)\n",
    "    with torch.no_grad():\n",
    "        losses = validate(val_loader, net, criterion, save_images, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
