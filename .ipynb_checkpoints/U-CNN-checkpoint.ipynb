{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from skimage import io, color\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    \n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict[b\"data\"]\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "def load_data():\n",
    "    # Load training data\n",
    "    train = np.reshape(unpickle('cifar-10-batches-py/data_batch_1'), (10000, 3, 32, 32))\n",
    "    train = np.append(train, np.reshape(unpickle('cifar-10-batches-py/data_batch_2'), (10000, 3, 32, 32)), 0)\n",
    "    train = np.append(train, np.reshape(unpickle('cifar-10-batches-py/data_batch_3'), (10000, 3, 32, 32)), 0)\n",
    "    train = np.append(train, np.reshape(unpickle('cifar-10-batches-py/data_batch_4'), (10000, 3, 32, 32)), 0)\n",
    "    train = np.append(train, np.reshape(unpickle('cifar-10-batches-py/data_batch_5'), (10000, 3, 32, 32)), 0)\n",
    "\n",
    "    # Convert to greyscale and LAB\n",
    "    train_grey = np.zeros((50000, 1, 32, 32))\n",
    "    train_lab  = np.zeros((50000, 3, 32, 32))\n",
    "    for i in range(0, len(train)):\n",
    "        grey = np.dot(train[i].transpose(1,2,0), [0.299, 0.587, 0.114])\n",
    "        lab = color.rgb2lab(train[i].transpose(1,2,0)).transpose(2,0,1)\n",
    "        train_grey[i][0] = grey\n",
    "        train_lab[i] = lab\n",
    "    # Convert to 0-1 range to avoid tanh fuckery\n",
    "    train_lab = torch.tensor(train_lab/100).float()\n",
    "    train_grey = torch.tensor(train_grey/255).float()\n",
    "\n",
    "\n",
    "    # Load test and validation data\n",
    "    testvalid = np.reshape(unpickle('cifar-10-batches-py/test_batch'), (10000, 3, 32, 32))\n",
    "    valid = testvalid[0:9000]\n",
    "    test = testvalid[0:1000]\n",
    "\n",
    "    # Convert to greyscale and lab\n",
    "    valid_grey = np.zeros((9000, 1, 32, 32))\n",
    "    valid_lab = np.zeros((9000, 3, 32, 32))\n",
    "    for i in range(0, len(test)):\n",
    "        grey = np.dot(valid[i].transpose(1,2,0), [0.299, 0.587, 0.114])\n",
    "        valid_grey[i][0] = grey\n",
    "\n",
    "        lab = color.rgb2lab(valid[i].transpose(1,2,0)).transpose(2,0,1)\n",
    "        valid_lab[i] = lab\n",
    "    # Convert to 0-1 range to avoid tanh fuckery\n",
    "    valid_lab = torch.tensor(valid_lab/100).float()\n",
    "    valid_grey = torch.tensor(valid_grey/255).float()\n",
    "\n",
    "    # Convert to greyscale\n",
    "    test_grey = np.zeros((1000, 1, 32, 32))\n",
    "    for i in range(0, len(test)):\n",
    "        grey = np.dot(test[i].transpose(1,2,0), [0.299, 0.587, 0.114])\n",
    "        test_grey[i][0] = grey\n",
    "    # Convert to 0-1 range to avoid tanh fuckery\n",
    "    test_grey = torch.tensor(test_grey/255).float()\n",
    "    \n",
    "    return train_lab, train_grey, valid_lab, valid_grey, test, test_grey\n",
    "\n",
    "train_color, train_grey, valid_color, valid_grey, test_color, test_grey = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR_iterator():\n",
    "    def __init__(self, data_tuple, batch_size):\n",
    "        self.data_tuple = data_tuple\n",
    "        self.batch_size = batch_size\n",
    "        self.i = 0\n",
    "        self.iter = 0\n",
    "        self.iters = np.floor_divide(data_tuple[0].size(0), batch_size)\n",
    "        \n",
    "    def getNext(self):            \n",
    "        self.i += self.batch_size\n",
    "        self.iter += 1\n",
    "        res = (self.data_tuple[0][self.i:self.i +self.batch_size], self.data_tuple[1][self.i:self.i +self.batch_size])\n",
    "        return res\n",
    "    \n",
    "    def getIter(self):\n",
    "        return self.iter\n",
    "    \n",
    "    def getIters(self):\n",
    "        return self.iters\n",
    "    \n",
    "    def reset(self):\n",
    "        self.i = 0\n",
    "        self.iter = 0\n",
    "        \n",
    "def to_rgb(grayscale_input, ab_input, save_path=None, save_name=None):\n",
    "    '''Show/save rgb image from grayscale and ab channels\n",
    "       Input save_path in the form {'grayscale': '/path/', 'colorized': '/path/'}'''\n",
    "    plt.clf()  # clear matplotlib\n",
    "    color_image = ab_input.numpy()  # combine channels\n",
    "    color_image = color_image.transpose((1, 2, 0))  # rescale for matplotlib\n",
    "    color_image = color_image * 100\n",
    "    color_image = color.lab2rgb(color_image.astype(np.float64))\n",
    "    grayscale_input = grayscale_input.squeeze().numpy()\n",
    "    if save_path is not None and save_name is not None:\n",
    "        plt.imsave(arr=grayscale_input, fname='{}{}'.format(save_path['grayscale'], save_name), cmap='gray')\n",
    "        plt.imsave(arr=color_image, fname='{}{}'.format(save_path['colorized'], save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  loaded\n",
      "net initialised\n",
      "Starting training epoch 0\n",
      "Epoch: [0][24/781]\tTime 0.111 (0.192)\tData 0.000 (0.000)\tLoss 0.1517 (0.1571)\t\n",
      "Epoch: [0][49/781]\tTime 0.155 (0.181)\tData 0.000 (0.000)\tLoss 0.1289 (0.1459)\t\n",
      "Epoch: [0][74/781]\tTime 0.121 (0.193)\tData 0.000 (0.000)\tLoss 0.1146 (0.1380)\t\n",
      "Epoch: [0][99/781]\tTime 0.140 (0.187)\tData 0.000 (0.000)\tLoss 0.1140 (0.1316)\t\n",
      "Epoch: [0][124/781]\tTime 0.078 (0.180)\tData 0.000 (0.000)\tLoss 0.0985 (0.1260)\t\n",
      "Epoch: [0][149/781]\tTime 0.221 (0.175)\tData 0.000 (0.000)\tLoss 0.0933 (0.1208)\t\n",
      "Epoch: [0][174/781]\tTime 0.079 (0.168)\tData 0.000 (0.000)\tLoss 0.0903 (0.1160)\t\n",
      "Epoch: [0][199/781]\tTime 0.078 (0.160)\tData 0.000 (0.000)\tLoss 0.0817 (0.1116)\t\n",
      "Epoch: [0][224/781]\tTime 0.091 (0.155)\tData 0.000 (0.000)\tLoss 0.0693 (0.1076)\t\n",
      "Epoch: [0][249/781]\tTime 0.084 (0.154)\tData 0.000 (0.000)\tLoss 0.0658 (0.1039)\t\n",
      "Epoch: [0][274/781]\tTime 0.115 (0.154)\tData 0.000 (0.000)\tLoss 0.0643 (0.1005)\t\n",
      "Epoch: [0][299/781]\tTime 0.257 (0.155)\tData 0.000 (0.000)\tLoss 0.0567 (0.0973)\t\n",
      "Epoch: [0][324/781]\tTime 0.251 (0.159)\tData 0.000 (0.000)\tLoss 0.0581 (0.0943)\t\n",
      "Epoch: [0][349/781]\tTime 0.162 (0.162)\tData 0.000 (0.000)\tLoss 0.0548 (0.0914)\t\n",
      "Epoch: [0][374/781]\tTime 0.158 (0.163)\tData 0.000 (0.000)\tLoss 0.0486 (0.0889)\t\n",
      "Epoch: [0][399/781]\tTime 0.084 (0.161)\tData 0.000 (0.000)\tLoss 0.0513 (0.0865)\t\n",
      "Epoch: [0][424/781]\tTime 0.161 (0.163)\tData 0.000 (0.000)\tLoss 0.0441 (0.0841)\t\n",
      "Epoch: [0][449/781]\tTime 0.088 (0.162)\tData 0.000 (0.000)\tLoss 0.0438 (0.0818)\t\n",
      "Epoch: [0][474/781]\tTime 0.274 (0.163)\tData 0.000 (0.000)\tLoss 0.0410 (0.0797)\t\n",
      "Epoch: [0][499/781]\tTime 0.152 (0.165)\tData 0.000 (0.000)\tLoss 0.0373 (0.0777)\t\n",
      "Epoch: [0][524/781]\tTime 0.218 (0.167)\tData 0.000 (0.000)\tLoss 0.0324 (0.0757)\t\n",
      "Epoch: [0][549/781]\tTime 0.125 (0.168)\tData 0.000 (0.000)\tLoss 0.0311 (0.0737)\t\n",
      "Epoch: [0][574/781]\tTime 0.077 (0.167)\tData 0.000 (0.000)\tLoss 0.0298 (0.0719)\t\n",
      "Epoch: [0][599/781]\tTime 0.252 (0.167)\tData 0.000 (0.000)\tLoss 0.0262 (0.0701)\t\n",
      "Epoch: [0][624/781]\tTime 0.107 (0.167)\tData 0.000 (0.000)\tLoss 0.0275 (0.0684)\t\n",
      "Epoch: [0][649/781]\tTime 0.112 (0.166)\tData 0.000 (0.000)\tLoss 0.0267 (0.0669)\t\n",
      "Epoch: [0][674/781]\tTime 0.119 (0.165)\tData 0.000 (0.000)\tLoss 0.0217 (0.0653)\t\n",
      "Epoch: [0][699/781]\tTime 0.237 (0.164)\tData 0.000 (0.000)\tLoss 0.0217 (0.0638)\t\n",
      "Epoch: [0][724/781]\tTime 0.190 (0.165)\tData 0.000 (0.000)\tLoss 0.0233 (0.0624)\t\n",
      "Epoch: [0][749/781]\tTime 0.113 (0.166)\tData 0.000 (0.000)\tLoss 0.0213 (0.0610)\t\n",
      "Epoch: [0][774/781]\tTime 0.115 (0.165)\tData 0.000 (0.000)\tLoss 0.0195 (0.0597)\t\n",
      "Finished training epoch 0\n",
      "Finished validation.\n",
      "Starting training epoch 1\n",
      "Epoch: [1][24/781]\tTime 0.076 (0.116)\tData 0.000 (0.000)\tLoss 0.0225 (0.0196)\t\n",
      "Epoch: [1][49/781]\tTime 0.094 (0.127)\tData 0.000 (0.000)\tLoss 0.0209 (0.0189)\t\n",
      "Epoch: [1][74/781]\tTime 0.158 (0.132)\tData 0.000 (0.000)\tLoss 0.0141 (0.0187)\t\n",
      "Epoch: [1][99/781]\tTime 0.232 (0.143)\tData 0.000 (0.000)\tLoss 0.0213 (0.0183)\t\n",
      "Epoch: [1][124/781]\tTime 0.123 (0.152)\tData 0.000 (0.000)\tLoss 0.0152 (0.0180)\t\n",
      "Epoch: [1][149/781]\tTime 0.186 (0.165)\tData 0.000 (0.000)\tLoss 0.0192 (0.0178)\t\n",
      "Epoch: [1][174/781]\tTime 0.181 (0.171)\tData 0.000 (0.000)\tLoss 0.0170 (0.0175)\t\n",
      "Epoch: [1][199/781]\tTime 0.248 (0.173)\tData 0.000 (0.000)\tLoss 0.0176 (0.0174)\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2e0cd9058d36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-2e0cd9058d36>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, model, criterion, optimiser, epoch)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Compute gradient and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Unet, self).__init__()\n",
    "        #Convolution and deconvolution\n",
    "        self.conv1 = nn.Conv2d(1, 4, (4, 4), stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(4, 8, (4, 4), stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(8, 16, (4, 4), stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(16, 32, (4, 4), stride=2, padding=1)\n",
    "        #self.conv5 = nn.Conv2d(3, 3, (4, 4), stride=2, padding=1)\n",
    "        self.deconv1 = nn.ConvTranspose2d(32, 16, (4, 4), stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(32, 8, (4, 4), stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(16, 4, (4, 4), stride=2, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(8, 4, (4, 4), stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(4, 3, (1, 1))\n",
    "        \n",
    "        #Batchnorm\n",
    "        self.conv1_bnorm = nn.BatchNorm2d(4)\n",
    "        self.conv2_bnorm = nn.BatchNorm2d(8)\n",
    "        self.conv3_bnorm = nn.BatchNorm2d(16)\n",
    "        self.conv4_bnorm = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.deconv1_bnorm = nn.BatchNorm2d(16)\n",
    "        self.deconv2_bnorm = nn.BatchNorm2d(8)\n",
    "        self.deconv3_bnorm = nn.BatchNorm2d(4)\n",
    "        self.deconv4_bnorm = nn.BatchNorm2d(4)\n",
    "    \n",
    "    def forward(self, x32):\n",
    "        # Contraction\n",
    "        x16 = F.leaky_relu(self.conv1(x32), 0.2)\n",
    "        x16 = self.conv1_bnorm(x16)\n",
    "        \n",
    "        x8 = F.leaky_relu(self.conv2(x16), 0.2)\n",
    "        x8 = self.conv2_bnorm(x8)\n",
    "        \n",
    "        x4 = F.leaky_relu(self.conv3(x8), 0.2)\n",
    "        x4 = self.conv3_bnorm(x4)\n",
    "        \n",
    "        x2 = F.leaky_relu(self.conv4(x4), 0.2)\n",
    "        x2 = self.conv4_bnorm(x2)\n",
    "        \n",
    "        \n",
    "        # Expansion\n",
    "        x = F.relu(self.deconv1(x2))\n",
    "        x = self.deconv1_bnorm(x)\n",
    "        x4 = torch.cat((x,x4), 1)\n",
    "        \n",
    "        x = F.relu(self.deconv2(x4))\n",
    "        x = self.deconv2_bnorm(x)\n",
    "        x8 = torch.cat((x,x8), 1)\n",
    "        \n",
    "        x = F.relu(self.deconv3(x8))\n",
    "        x = self.deconv3_bnorm(x)\n",
    "        x16 = torch.cat((x,x16), 1)\n",
    "        \n",
    "        x = F.relu(self.deconv4(x16))\n",
    "        x = self.deconv4_bnorm(x)\n",
    "        \n",
    "        # cross-channel parametric pooling\n",
    "        # CHECK IF TANH IS A GOOD IDEA???\n",
    "        x = F.tanh(self.conv5(x))\n",
    "        #x = self.conv5(x)\n",
    "        return x\n",
    "\n",
    "# To track training loss\n",
    "class AverageMeter(object):\n",
    "    '''A handy class from the PyTorch ImageNet tutorial'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def write_results_to_file(file_dir, file_name, data):\n",
    "    file = open(file_dir + os.path.sep + file_name, 'a')\n",
    "    if isinstance(data, str):\n",
    "        file.write(data + '\\n')\n",
    "    else:\n",
    "        for line in data:\n",
    "            file.write(line + '\\n')\n",
    "    file.close()\n",
    "\n",
    "def train_model(train_loader, model, criterion, optimiser, epoch):\n",
    "    print('Starting training epoch {}'.format(epoch))\n",
    "    model.train()\n",
    "    \n",
    "    dir_name = \"res\"\n",
    "    file_name = \"intermediate\"\n",
    "    \n",
    "    # Prepare value counters and timers\n",
    "    batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    for i in range(0, train_loader.getIters()):\n",
    "        \n",
    "        (input_gray, target) = train_loader.getNext()\n",
    "        # use gpu\n",
    "        if use_gpu: input_gray, target = input_gray.cuda(), target.cuda()\n",
    "        \n",
    "        # Record load time data\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        # Run forward pass\n",
    "        output_ab = model(input_gray)\n",
    "        loss = criterion(output_ab, target)\n",
    "        losses.update(loss.item(), input_gray.size(0))\n",
    "        \n",
    "        # Compute gradient and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "         # Record time to do forward and backward passes\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        # Print model accuracy -- in the code below, val refers to value, not validation\n",
    "        if train_loader.getIter() % 25 == 0:\n",
    "            stats = (\n",
    "                'Epoch: [{0}][{1}/{2}]\\tTime {batch_time.val:.3f} ({batch_time.avg:.3f})\\tData {data_time.val:.3f} ({'\n",
    "                'data_time.avg:.3f})\\tLoss {loss.val:.4f} ({loss.avg:.4f})\\t').format(\n",
    "                epoch, i, train_loader.getIters(), batch_time=batch_time,\n",
    "                data_time=data_time, loss=losses)\n",
    "            print(stats)\n",
    "            write_results_to_file(dir_name, file_name, stats)\n",
    "            \n",
    "    train_loader.reset()\n",
    "\n",
    "    print('Finished training epoch {}'.format(epoch))\n",
    "    \n",
    "def validate(val_loader, model, criterion, save_images, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare value counters and timers\n",
    "    batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    already_saved_images = False\n",
    "    for i in range(0, 1):\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        (input_gray, target) = val_loader.getNext()\n",
    "        # use gpu\n",
    "        if use_gpu: input_gray, target = input_gray.cuda(), target.cuda()\n",
    "\n",
    "        # Run model and record loss\n",
    "        output_ab = model(input_gray)  # throw away class predictions\n",
    "        loss = criterion(output_ab, target)\n",
    "        losses.update(loss.item(), input_gray.size(0))\n",
    "\n",
    "        # Save images to file\n",
    "        if save_images and not already_saved_images:\n",
    "            already_saved_images = True\n",
    "            for j in range(min(len(output_ab), 10)):  # save at most 5 images\n",
    "                save_path = {'grayscale': 'res/grey/', 'colorized': 'res/color/'}\n",
    "                save_name = 'img-{}-epoch-{}.jpg'.format(i * val_loader.batch_size + j, epoch)\n",
    "                to_rgb(input_gray[j].cpu(), ab_input=output_ab[j].detach().cpu(), save_path=save_path,\n",
    "                       save_name=save_name)\n",
    "\n",
    "        # Record time to do forward passes and save images\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "    val_loader.reset()\n",
    "    print('Finished validation.')\n",
    "    return losses.avg\n",
    "\n",
    "# Load and process the inputs and targets\n",
    "#targets, inputs, valid_targets, valid_inputs, test_inputs = load_data()\n",
    "print(\"data  loaded\")\n",
    "\n",
    "# Make net\n",
    "net = Unet()\n",
    "print(\"net initialised\")\n",
    "\n",
    "# Ensure res directory exists\n",
    "os.makedirs('res', exist_ok=True)\n",
    "os.makedirs('res/grey', exist_ok=True)\n",
    "os.makedirs('res/color/', exist_ok=True)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0002, betas=(0.5,0.999))\n",
    "criterion = torch.nn.MSELoss()\n",
    "epochs = 50\n",
    "save_images = True\n",
    "\n",
    "train_loader = CIFAR_iterator((train_grey, train_color), 64)\n",
    "val_loader = CIFAR_iterator((valid_grey, valid_color), 64)\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train_model(train_loader, net, criterion, optimizer, epoch)\n",
    "    with torch.no_grad():\n",
    "        losses = validate(val_loader, net, criterion, save_images, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
